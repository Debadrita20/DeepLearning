# -*- coding: utf-8 -*-
"""DL_Assg_Code_25.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rk5QntJnA8xC7wnbhGxqWIEvntvLH4fN
"""


import pandas as pd
from sklearn.model_selection import train_test_split
import sklearn.preprocessing
import re
import string
from nltk import word_tokenize
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras import Model, metrics
from keras.layers import GlobalMaxPooling1D, MaxPooling1D, MaxPooling2D
from keras.layers.convolutional import Conv1D, Conv2D
# import cleantext
import numpy as np

import nltk
nltk.download('punkt')
nltk.download('stopwords')

path = "/content/drive/MyDrive/Dataset.csv"
data = pd.read_csv(path)
outputs = data['airline_sentiment']
inputs = data['text']

training, test, results_training, results_test = train_test_split(inputs,outputs,test_size=0.2,random_state=150)

# preprocessing each tweet

def clean(st):
  ans=''
  ans=re.sub('@\w*','',st)
  return ans
  

# changing to lower case
def lower(toks):
  return [tok.lower() for tok in toks]

# removing stop words
def rem_stop_wrds(toks):
  list_stop=stopwords.words('english')
  return [wrd for wrd in toks if wrd not in list_stop]




# cleaning tweets
username_remd_tweets=training.apply(lambda x: clean(x))

tokens_tweets=[word_tokenize(txt) for txt in username_remd_tweets]
tokens_tweets=[lower(tkn) for tkn in tokens_tweets]
tokens_tweets=[rem_stop_wrds(tkn) for tkn in tokens_tweets]
print(tokens_tweets)
vocab=set()
for tw in tokens_tweets:
  for w in tw:
    for ch in w:
      # print(ch)
      vocab.add(ch)
#print(len(vocab))

vocab_list=list(vocab)
max_length=150
onehot_encoded_inputs=[]
for tw in tokens_tweets:
  tw_code=[]
  for w in tw:
    for ch in w:
      p=vocab_list.index(ch)
      ll=(len(vocab_list)+1)*[0]
      ll[p]=1
      tw_code.append(ll)
  onehot_encoded_inputs.append(tw_code)
'''print(len(tokens_tweets))
print(len(onehot_encoded_inputs))
print(onehot_encoded_inputs[0])
print(max_length)
print(len(onehot_encoded_inputs[0]))'''
'''for i in range(11712):
  if not len(onehot_encoded_inputs[i])<=116:
    print(tokens_tweets[i])'''
# padding the tweets
for i in range(len(onehot_encoded_inputs)):
  if len(onehot_encoded_inputs[i])<max_length:
    for _ in range(max_length-len(onehot_encoded_inputs[i])):
      onehot_encoded_inputs[i].append((len(vocab_list)+1)*[0])
#print(len(onehot_encoded_inputs[0]))

label_encoder = sklearn.preprocessing.LabelEncoder()
integer_encoded_out = label_encoder.fit_transform(results_training)
#print(results_training)
print(integer_encoded_out)
onehot_encoder = sklearn.preprocessing.OneHotEncoder(sparse=False)
integer_encoded = integer_encoded_out.reshape(len(integer_encoded_out), 1)
onehot_encoded_outputs = onehot_encoder.fit_transform(integer_encoded)
print(len(onehot_encoded_outputs[0]))

from keras.layers.regularization.spatial_dropout1d import Dropout
def CNN(train_x,train_y,len_out):
  model=Sequential()
  model.add(Conv1D(filters=256,kernel_size=7,activation='relu'))
  #model.add(Dropout(0.1))
  # model.add(MaxPooling1D(pool_size=2,strides=2,padding='valid'))
  
  model.add(Conv1D(filters=128,kernel_size=5,activation='relu'))
  #model.add(Dropout(0.5))
  model.add(MaxPooling1D(pool_size=2,strides=1,padding='valid'))
  model.add(Conv1D(filters=256,kernel_size=3,activation='relu'))
  model.add(Dropout(0.1))
  model.add(MaxPooling1D(pool_size=2,strides=2,padding='valid'))
  #model.add(Conv1D(filters=64,kernel_size=3,activation='relu'))
  #model.add(MaxPooling1D(pool_size=2,strides=2,padding='valid'))
  model.add(Flatten())
  model.add(Dense(512,activation='relu'))
  #model.add(Dropout(0.1))
  model.add(Dense(128,activation='relu'))
  #model.add(Dropout(0.1))
  model.add(Dense(len_out,activation='softmax'))
  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=[metrics.CategoricalAccuracy()])
  model.fit(train_x,train_y,epochs=10,batch_size=128,verbose=2)
  
  model.save('/content/drive/MyDrive/modelseq57.h5')
  model.summary()
  return model

train_x=np.array(onehot_encoded_inputs,dtype='float32')
train_y=np.array(onehot_encoded_outputs,dtype='float32')
model=CNN(train_x,train_y,len(onehot_encoded_outputs[0]))

# preprocessing test tweets
username_remd_tweetst=test.apply(lambda x: clean(x))
tokens_tweetst=[word_tokenize(txt) for txt in username_remd_tweetst]
tokens_tweetst=[lower(tkn) for tkn in tokens_tweetst]
tokens_tweetst=[rem_stop_wrds(tkn) for tkn in tokens_tweetst]
# tokens_tweets=[rem_1st_wrd(tkn) for tkn in tokens_tweets]
#print(tokens_tweetst)

onehot_encoded_inputst=[]
for tw in tokens_tweetst:
  tw_code=[]
  for w in tw:
    for ch in w:
      
      ll=(len(vocab_list)+1)*[0]
      if ch in vocab_list:
        ll[vocab_list.index(ch)]=1
      else: # unknown word
        ll[len(vocab_list)]=1
      tw_code.append(ll)
  onehot_encoded_inputst.append(tw_code)
'''print(len(tokens_tweetst))
print(len(onehot_encoded_inputst))
print(onehot_encoded_inputst[0])
print(max_length)
print(len(onehot_encoded_inputst[0]))'''
'''for i in range(2928):
  if not len(onehot_encoded_inputst[i])<=116:
    print(tokens_tweetst[i])'''
# padding the tweets
for i in range(len(onehot_encoded_inputst)):
  if len(onehot_encoded_inputst[i])<max_length:
    for _ in range(max_length-len(onehot_encoded_inputst[i])):
      onehot_encoded_inputst[i].append((len(vocab_list)+1)*[0])
#print(len(onehot_encoded_inputst[0]))
# label_encoder = sklearn.preprocessing.LabelEncoder()
#print(results_test)
integer_encoded_outt = label_encoder.transform(results_test)
print(integer_encoded_outt)
# onehot_encoder = sklearn.preprocessing.OneHotEncoder(sparse=False)
integer_encodedt = integer_encoded_outt.reshape(len(integer_encoded_outt), 1)
onehot_encoded_outputst = onehot_encoder.transform(integer_encodedt)
print(onehot_encoded_outputst)

from sklearn import metrics as mets

test_x=np.array(onehot_encoded_inputst)
test_y=np.array(onehot_encoded_outputst)
predictions=model.predict(test_x,verbose=2)
print(predictions)
test_y=np.argmax(test_y,axis=1)
predictions=np.argmax(predictions,axis=1)
ac = mets.accuracy_score(test_y,predictions)
print("Accuracy= ",ac)
print("Precision= ",mets.precision_score(test_y,predictions,average=None))
print("Recall= ",mets.recall_score(test_y,predictions,average=None))
print("F1 Score= ",mets.f1_score(test_y,predictions,average=None))
